---
title: "INTENTO 2 - LIMPIEZA, FACTORIAL Y CLUSTER"
author: "Ariana Zuazo"
date: "2024-11-16"
output: html_document
---

***INDICACIONES:***

*LIMPIAR LA DATA*, DONDE TODOS LOS CASOS TIENEN DATOS COMPLETOS

HACER EL *ANÁLISIS FACTORIAL* PARA DETERMINAR QUE VARIABLES PODEMOS USAR EN EL CLUSTER

HACER EN *ANÁLISIS CONGLOMERADO* CON LAS VARIABLES QUE YA SE HAN DETERMINADO ANTES

-----------------------------------------------------------------------------------------

## LIMPIEZA DE DATA

```{r}
library(rio)
library(tidyverse)
rm(list = ls()) # limpiar memoria
data_ori = import("LA REAL REAL DATA .xlsx")
data_ori <- data_ori %>%
  mutate(
   calsijud = as.numeric(calsijud),
    i.gini = as.numeric(i.gini))

names(data_ori)
```

```{r}
data_ok <- data_ori[complete.cases(data_ori), ]
```

## ANALISIS FACTORIAL

*SELECCION DE VARIABLES:*

```{r}
dontselect=c("paises","region","idh.2022")
select=setdiff(names(data_ok),dontselect) 
data_num=data_ok[,select]

# usaremos:
library(magrittr)
head(data_num,10)%>%
    rmarkdown::paged_table()
```

CALCULAMOS LA CORRELACION ENTRE LAS VARIABLES:

```{r}
library(polycor)
corMatrix=polycor::hetcor(data_num)$correlations
round(corMatrix,2)
```

CONTEMPLAMOS EN UN GRÁFICO

```{r}
## install.packages("ggcorrplot")
library(ggcorrplot)

ggcorrplot(corMatrix)
```

PASOS QUE REQUIERE EL EFA (ANALISIS FACTORIAL EXPLORATORIO)

PASO 01: VERIFICAR SI LOS DATOS PERMITEN FACTORIZAR

```{r}
library(psych)
psych::KMO(corMatrix)
```

PASO 02: VERIFICAR SI LA MATRIZ DE CORRELACIONES ES LA ADECUADA

DOS PRUEBAS: (EN AMBOS TIENEN QUE SALIR FALSE)

MATRIZ IDENTIDAD:
```{r}
cortest.bartlett(corMatrix,n=nrow(data_num))$p.value>0.05
```

MATRIZ SINGULAR:

```{r}
## install.packages("matrixcalc")
library(matrixcalc)

is.singular.matrix(corMatrix)
```

PASO 03: DETERMINAR EN CUANTOS FACTORES PODEMOS REDIMENSIONAR LA DATA

```{r}
fa.parallel(data_num, fa = 'fa',correct = T,plot = F)
```

SE SUGIERE 1

PASO 04: REDIMENSIONAR A NUMERO MENOR DE FACTORES

RESULTADO INICIAL: 

```{r}
##install.packages("GPArotation")
library(GPArotation)
resfa <- fa(data_num,
            nfactors = 1,
            cor = 'mixed',
            rotate = "varimax", #oblimin?
            fm="minres")
print(resfa$loadings)
```
```{r}
print(resfa$loadings,cutoff = 0.5)
```

RESULTADO VISUAL:

```{r}
fa.diagram(resfa,main = "Resultados del EFA")
```

PASO 05: EVALUANDO RESULTADO OBTENIDO

¿Qué variables aportaron más a los factores?

```{r}
sort(resfa$communality)
```

¿Qué variables contribuyen a la construcción de más de un factor?

```{r}
sort(resfa$complexity)
```

¿Tucker Lewis > 0.9?

```{r}
resfa$TLI
```

¿RMS cerca a cero?

```{r}
resfa$rms
```

¿RMSEA cerca a cero?

```{r}
resfa$RMSEA
```

¿BIC?

```{r}
resfa$BIC
```

OBTENCIÓN DE INDICES

```{r}
as.data.frame(resfa$scores)%>%head()
```

## ANÁLISIS CONGLOMERADO

```{r}
names(data_ok)
```

TRANSFORMACIÓN DE DATA:

Para este ejercicio sólo usaremos los componentes del IDH. La distribución de los componentes del IDH podemos verla en la Figura 2.1.

```{r}
boxplot(data_ok[,c(4:11)],horizontal = F,las=2,cex.axis = 0.5)
```

Como primera estrategia cambiemos sus rangos. Elijamos un rango del 0 al 1, cuyo resultado se ve en la Figura

```{r}
library(BBmisc)
boxplot(normalize(data_ok[,c(4:11)],method='range',range=c(0,10)))
```

TIPIFICACIÓN DE LAS VARIABLES

```{r}
data_tipificada <- as.data.frame(scale(data_num))
print(data_tipificada)
```

BOXPLOT:

```{r}
boxplot(normalize(data_tipificada,method='standardize'))
```

NOS QUEDAMOS CON OLA SEGUNDA OPCION:

```{r}
data_tipificada=normalize(data_tipificada,method='standardize')
```

CORRELACIÓN:

Veamos correlaciones entre estas variables tipificadas:

```{r}
cor(data_tipificada, use = "complete.obs")
```

PREPARACIÓN DE LOS DATOS PARA LA CLUSTERIZACIÓN:

No podemos usar la columna paises en la clusterización, pero tampoco debemos perderla, por lo que se recomienda usar esos nombres en lugar del nombre de fila.

```{r}
dataClus=data_tipificada
row.names(dataClus)=data_ok$paises
```

Ya con los datos en el objeto dataClus, calculemos la matriz de distancias entre los casos (paises):

```{r}
library(cluster)
g.dist = daisy(dataClus, metric="gower")
```

Usaremos la distancia Gower útil cuando las variables (columnas) están de diversos tipos de escalas.

PROCESOS DE CLUSTERIZACIÓN:

*PARTICIÓN*: Busca partir los casos en grupos. FUNCION QUE SE USA **K-MEDOIDES O PAM**

LOS CLUSTERS SUGERIDOS POR PAM SON 3:

PASO 01: DECIDIR LA CANTIDAD DE CLUSTERS


```{r}
library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
```

PASO 02: CLUSTERIZAR VÍA PAM (K-MEDOIDES)

La técnica de k-medoides se implementa en la función pam. Esta función retorna diversos valores, en este caso crearemos una columna con la etiqueta del **cluster**. Usemos la sugerencia del gráfico del PASO 01, y hallamos:

```{r}
library(kableExtra)
library(cluster)
set.seed(123)
res.pam=pam(g.dist,3,cluster.only = F)

#nueva columna
dataClus$pam=res.pam$cluster

# ver

head(dataClus,15)%>%kbl()%>%kable_styling()
```

PASO 03: EVALUANDO EL USO DE PAM

Una manera práctica de ver el desempeño del algoritmo es calcular las silhouettes. Para el caso reciente, veamos la Figura 5.2.

```{r}
fviz_silhouette(res.pam,print.summary = F)
```

La Figura 5.2 muestra barras, donde cada una es un país (caso). *Mientras más alta la barra, la pertenencia a ese cluster es clara.* La barra negativa indica un país mal clusterizado. Para este caso, estos serían los mal clusterizados:

```{r}
silPAM=data.frame(res.pam$silinfo$widths)
silPAM$paises=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'paises']%>%sort()
poorPAM
```

PASO 04: VERIFICANDO ETIQUETA DE CLUSTERS

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ pam, data=dataClus,mean)
```

ORDEN ORIGINAL:

```{r}
original=aggregate(.~ pam, data=dataClus,mean)
original[order(original$i.criminalidad),]
```

Esas posiciones hay que usarlas para recodificar:

```{r}
dataClus$pam=dplyr::recode(dataClus$pam, `2` = 1, `1`=2,`3`=3)
```

Antes de continuar, guardemos la columna de PAM en la data integrada, y eliminemos la de dataClus.

```{r}
data_ok$pamIDHpoor=data_ok$paises%in%poorPAM
data_ok$pamIDH=as.ordered(dataClus$pam)
dataClus$pam=NULL
```

*JERARQUIZACIÓN*: CLUSTERIZAR POR ETAPAS, TIENE DOS FAMILAS.

**ESTRATEGIA AGLOMERATIVA**: CONSIDERA CLUSTER A CADA CASO (FILA) Y DESINTEGRARLO EN MINICLUSTERS. PENSARLO COMO UN BUTTOM-UP. FUNCION QUE SE USA: **AGNES**

PASO 02: DECIDIR LA CANTIDAD DE CLUSTER (SE UTILIZA EL ESTADISTICO GAP)

```{r}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

PASO 03: CLUSTERIZAR VIA AGNES

La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo aglomerativo se emplea usando **agnes**. El linkage será ward (aquí ward.D):

```{r}
set.seed(123)
library(factoextra)

res.agnes<- hcut(g.dist, k = 2,hc_func='agnes',hc_method = "ward.D")

dataClus$agnes=res.agnes$cluster

# ver

head(dataClus,15)%>%kbl()%>%kable_styling()
```

El dendograma de la Figura 5.4 nos muestra el proceso de conglomeración AGNES:

```{r}
# Visualize
fviz_dend(res.agnes, cex = 0.7, horiz = T,main = "")
```

El eje ‘Height’ nos muestra el “costo” de conglomerar: **mientras más corta la distancia mayor similitud y la conglomeracion es más rápida.**

PASO 04: EVALUANDO EL USO DE AGNES (SE UTILIZA SILHOUETTES)

La Figura 5.5 nos muestra las silhouettes para AGNES.

```{r}
fviz_silhouette(res.agnes,print.summary = F)
```

Nótese que también se presentan valores mal clusterizados. Los identificados son estos:

```{r}
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$paises=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'paises']%>%sort()
poorAGNES
```

PASO 05: VERIFICANDO LAS ETIQUETAS DE CLUSTERS

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ agnes, data=dataClus,mean)
```

ORDEN ORIGINAL:

```{r}
original=aggregate(.~ agnes, data=dataClus,mean)
original[order(original$i.criminalidad),]
```

Esas posiciones hay que usarlas para recodificar:

```{r}
dataClus$agnes=dplyr::recode(dataClus$agnes, `2` = 1, `1`=2)
```

LUEGO DE RECODIFICAR. Guardemos la columna de AGNES en la data integrada, y eliminemosla de dataClus.

```{r}
data_ok$agnesIDHpoor=data_ok$paises%in%poorAGNES
data_ok$agnesIDH=as.ordered(dataClus$agnes)
dataClus$agnes=NULL
```


PASO 6: COMPARANDO LA ESTRATEGIA PARTICION Y AGLOMERACION

Veamos qué tanto se parece a la clasificación jerarquica a la de partición:

```{r}
# verificar recodificacion
table(data_ok$pamIDH,data_ok$agnesIDH,dnn = c('Particion','Aglomeracion'))
```

LA DIAGONAL SON LAS COINCIDENCIAS DE AMBAS ESTRATEGIAS (PARTICION Y AGLOMERACIÓN)

**ESTRATEGIA DIVISIVOS**: COMIENZA EN QUE TODO ES UN CLUSTER Y SE VA DIVIDIENDO. PENSARLO COMO UN **TOP-DOWN**. FUNCION QUE SE UTILIZA: **DIANA**

PASO 01: DECIDIR LA CANTIDAD DE CLUSTERS

La Figura 5.6 sirve para determinar la cantidad de clusters a solicitar (**usando el estadístico gap**).

```{r}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana")
```

PASO 02: CLUSTERIZAR VIA DIANA

La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo divisivo se emplea usando **diana**. Aquí una muestra del resultado:

```{r}
set.seed(123)
res.diana <- hcut(g.dist, k = 3,hc_func='diana')
dataClus$diana=res.diana$cluster
# veamos
head(dataClus,15)%>%kbl%>%kable_styling()
```

El dendograma de la Figura 5.7 nos muestra el proceso de conglomeración AGNES:

```{r}
# Visualize
fviz_dend(res.diana, cex = 0.7, horiz = T, main = "")
```

PASO 03: EVALUANDO EL USO DE DIANA

La Figura 5.8 nos muestra las silhouettes para DIANA.

```{r}
fviz_silhouette(res.diana,print.summary = F)
```

Nótese que también se presentan valores mal clusterizados. Los identificados son estos:

```{r}
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$paises=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'paises']%>%sort()
poorDIANA
```

PASO 04: VERIFICANDO ETIQUETA

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ diana, data=dataClus,mean)
```

Aquí vemos que las etiquetas **no muestran un orden**. Este sería el orden:

```{r}
original=aggregate(.~ diana, data=dataClus,mean)
original[order(original$i.criminalidad),]
```

Esas posiciones hay que usarlas para recodificar:

```{r}
dataClus$diana=dplyr::recode(dataClus$diana, `2` = 1, `1`=2,`3`=3)
```

Guardemos la columna de DIANA en la data integrada, y eliminemosla de dataClus.

```{r}
data_ok$dianaIDHpoor=data_ok$paises%in%poorDIANA
data_ok$dianaIDH=as.ordered(dataClus$diana)
dataClus$diana=NULL
```

VISUALIZACIÓN COMPARATIVA

Vamos a usar la matriz de distancia para darle a cada país una coordenada, tal que la distancia entre esos paises se refleje en sus posiciones. Eso requiere una técnica que proyecte las dimensiones originales en un plano bidimensional. Para ello usaremos la técnica llamada escalamiento multidimensional. Veamos algunas coordenadas.

```{r}
# k es la cantidad de dimensiones
proyeccion = cmdscale(g.dist, k=2,add = T)
head(proyeccion$points,20)
```


Habiendo calculado la proyeccción, recuperemos las coordenadas del mapa del mundo basado en nuestras dimensiones nuevas:

```{r}
# data frame prep:
data_ok$dim1 <- proyeccion$points[,1] #fila uno
data_ok$dim2 <- proyeccion$points[,2] #fila dos
```

MAPA CON LAS COORDENADAS:

```{r}
library(ggrepel)
base= ggplot(data_ok,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text_repel(size=3, max.overlaps = 50,min.segment.length = unit(0, 'lines'))
```


Coloreemos el mapa anterior segun el cluster al que corresponden.

GRAFICA DE PAM:

```{r}
# solo paises mal clusterizados
PAMlabels=ifelse(data_ok$pamIDHpoor,data_ok$paises,'')

#base
base= ggplot(data_ok,aes(x=dim1, y=dim2))  +
    scale_color_brewer(type = 'qual',palette ='Dark2'  ) + labs(subtitle = "Se destacan los países mal clusterizados")

pamPlot=base + geom_point(size=3, 
                          aes(color=pamIDH))  + 
        labs(title = "PAM") 
# hacer notorios los paises mal clusterizados
pamPlot + geom_text_repel(size=4,
                          aes(label=PAMlabels),
                          max.overlaps = 50,
                          min.segment.length = unit(0, 'lines'))
```

GRAFICA CON AGNES

```{r}
# solo paises mal clusterizados
AGNESlabels=ifelse(data_ok$agnesIDHpoor,data_ok$paises,'')

agnesPlot=base + geom_point(size=3, 
                            aes(color=as.factor(agnesIDH))) +
          labs(title = "AGNES") 
# hacer notorios los paises mal clusterizados
agnesPlot + geom_text_repel(size=4,
                            aes(label=AGNESlabels),
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```

GRAFICA CON DIANA

```{r}
# solo paises mal clusterizados
DIANAlabels=ifelse(data_ok$dianaIDHpoor,data_ok$paises,'')

dianaPlot=base + geom_point(size=3,
                            aes(color=dianaIDH)) + 
          labs(title = "DIANA")

# hacer notorios los paises mal clusterizados
dianaPlot + geom_text_repel(size=4,
                            aes(label=DIANAlabels), 
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```

